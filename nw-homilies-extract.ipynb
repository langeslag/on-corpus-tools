{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddac0928",
   "metadata": {},
   "source": [
    "# _Norwegian Homily Book_ Plaintext Corpus Generation\n",
    "\n",
    "HTML from [heimskringla.no](https://heimskringla.no/wiki/Gammel_norsk_Homiliebog) following [Unger 1864](https://archive.org/details/gammelnorskhomi00ungegoog). A recent transcription of this manuscript is available in [Menota](https://www.menota.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b8d58b5-8877-4964-be5d-62a4b928dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re,json,copy\n",
    "from urllib.request import urlretrieve\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "Path(\"nhb/raw\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"nhb/clean\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"nhb/plaintext\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"nhb/nlp\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75bdd40-4503-49ab-8a72-026937be0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = ['alcuin', 'hom', 'olafr', 'visio', 'paternoster', 'anhang1', 'anhang2']\n",
    "\n",
    "# TODO: normalize remaining long vowels?\n",
    "# TODO: normalize ꜵ? Problem is it sometimes represents a, sometimes á, sometimes ǫ; so turn into a?\n",
    "def normalize(target):\n",
    "    matrix = {\n",
    "        'j': 'i',\n",
    "        'v': 'u',\n",
    "        'ę': 'æ',\n",
    "        'ẻ': 'æ',\n",
    "        'ỏ': 'ǫ',\n",
    "        'đ': 'ð'\n",
    "    }\n",
    "    for k,v in matrix.items():\n",
    "        target = target.replace(k, v)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd97b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = {\n",
    "        'alcuin': 'https://heimskringla.no/wiki/Cve%C3%B0iusending_Alquini_diaconi',\n",
    "        'hom': 'https://heimskringla.no/wiki/Homilier',\n",
    "        'olafr': 'https://heimskringla.no/wiki/In_die_sancti_Olaui_regis_et_martiris',\n",
    "        'visio': 'https://heimskringla.no/wiki/Visio_sancti_Pauli_apostoli',\n",
    "        'paternoster': 'https://heimskringla.no/wiki/Fa%C3%B0er_var',\n",
    "        'anhang1': 'https://heimskringla.no/wiki/Anhang_I_(Gammel_norsk_Homiliebog)',\n",
    "        'anhang2': 'https://heimskringla.no/wiki/Anhang_II_(Gammel_norsk_Homiliebog)'\n",
    "        }\n",
    "\n",
    "path = 'nhb/raw'\n",
    "for title, url in remote.items():\n",
    "    local = os.path.join(path, title) + '.html'\n",
    "    if not(os.path.exists(local) and os.path.getsize(local) > 0):\n",
    "        urlretrieve(url, local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ed696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in documents:\n",
    "    raw_html = 'nhb/raw/' + book + '.html'\n",
    "    clean_html = 'nhb/clean/' + book + '.html'\n",
    "    text_file = 'nhb/plaintext/' + book + '.txt'\n",
    "    nlp_file = 'nhb/nlp/' + book + '.txt'\n",
    "    with open(raw_html) as html_doc:\n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "        # Since heimskringla.no lacks a class name for the main text that we could select for,\n",
    "        # we'll just nuke all the unwanted nodes instead:\n",
    "        unwanted_elements = ['title', 'script', 'meta', 'link', 'center', 'a', 'sup', 'table']\n",
    "        unwanted_classes = ['.mw-references-wrap', '.printfooter', '.catlinks', '.visualClear', '.mw-indicators mw-body-content', '.toccolours', '.thumb tright', '.thumbinner', '.thumbcaption', '.magnify'] # spaced entries aren't caught\n",
    "        unwanted_ids = ['mw-page-base', 'mw-head-base', 'mw-navigation', 'toc', 'siteSub', 'contentSub', 'jump-to-nav', 'mw-parser-output', 'firstHeading', 'footer', 'footer-info-lastmod']\n",
    "        #unwanted_attribute_elements = ['div, span']\n",
    "        for element in unwanted_elements:\n",
    "            for match in soup.find_all(element):\n",
    "                match.decompose()\n",
    "        for attr_class in unwanted_classes:\n",
    "            for match in soup.css.select(attr_class):\n",
    "                match.decompose()\n",
    "        for match in soup.select('div[class^=toclimit]'):\n",
    "            match.decompose()\n",
    "        for match in soup.select('div[class^=thumb]'):\n",
    "            match.decompose()\n",
    "        for match in soup.select('div[class^=mw-indicators]'):\n",
    "            match.decompose()\n",
    "        for identifier in unwanted_ids:\n",
    "            for match in soup.find_all(id=identifier):\n",
    "                match.decompose()\n",
    "        for match in soup.find_all(\"b\", string=\"Fotnoter\"):\n",
    "            match.decompose()\n",
    "        for match in soup.find_all(\"b\", string=\"Fotnoter:\"):\n",
    "            match.decompose()\n",
    "        for match in soup.find_all(\"i\", string=\"Innskudd fra Andre Krønikebok, kapittel 20.\"):\n",
    "            match.decompose()\n",
    "        for match in soup.find_all(\"p\", string=\"()\"):\n",
    "            match.decompose()\n",
    "        for match in soup.find_all(\"i\"):\n",
    "            match.insert(0, '{')\n",
    "            match.append('}')\n",
    "        all_divs = soup.find_all('div')\n",
    "        for div in all_divs:\n",
    "            for element in div(string=lambda string: isinstance(string, Comment)):\n",
    "                element.extract()\n",
    "        soup.head.clear()\n",
    "        with open(clean_html, 'w') as file:\n",
    "            file.write(str(soup))\n",
    "        # Here is where I delete rubrics and chapter numbers\n",
    "        # to leave only the text for NLP evaluation.\n",
    "        # Comment out span to reintroduce chapter headings!\n",
    "        last_unwanted_elements = ['b', 'span']\n",
    "        for element in last_unwanted_elements:\n",
    "            for match in soup.find_all(element):\n",
    "                match.decompose()\n",
    "        with open(text_file, 'w') as file:\n",
    "            file.write(soup.get_text())\n",
    "        # Now reopen as plaintext to remove punctuation and blank lines:\n",
    "        unwanted_chars = ['.', ':', ';', '?', '!', '[', ']', '(', ')', '| ']\n",
    "        plaintext = open(text_file).readlines()\n",
    "        flattened = []\n",
    "        for line in plaintext:\n",
    "            for character in unwanted_chars:\n",
    "                line = line.replace(character, '')\n",
    "            line = line.replace('{', '[').replace('}', ']')\n",
    "            line = normalize(' '.join(line.lower().lstrip().split()))\n",
    "            if not re.match('^$', line):\n",
    "                flattened.append(line + '\\n')\n",
    "        with open(nlp_file, 'w') as f:\n",
    "            f.writelines(flattened)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
